{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMDx3i4Lxp8q5jw9YW9IoY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raghava-1845/NNDL-6/blob/main/exp1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([0.2, 0.4, 0.6])\n",
        "\n",
        "w = np.array([\n",
        "    [0.2, 0.1],\n",
        "    [0.4, 0.3],\n",
        "    [0.6, 0.5]\n",
        "])\n",
        "b = np.array([0.3, 0.4])\n",
        "\n",
        "z = np.dot(x, w) + b\n",
        "print(\"o/p\", z)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "a = sigmoid(z)\n",
        "print(\"activation\", a)\n",
        "\n",
        "w2 = np.array([[0.4],\n",
        "               [0.6]])\n",
        "b2 = np.array([0.2])\n",
        "\n",
        "z2 = np.dot(a, w2) + b2\n",
        "a2 = sigmoid(z2)\n",
        "print(\"final output\", a2)\n",
        "\n",
        "y = 0.8\n",
        "\n",
        "loss = (y - a2) ** 2\n",
        "print(\"loss\", loss)\n",
        "\n",
        "lr = 0.1\n",
        "\n",
        "dloss_da2 = -2 * (y - a2)\n",
        "da2_dz2 = a2 * (1 - a2)\n",
        "dz2_dw2 = a\n",
        "\n",
        "grad_w2 = dloss_da2 * da2_dz2 * dz2_dw2\n",
        "w2 = w2 - lr * grad_w2.reshape(2, 1)\n",
        "\n",
        "print(\"updated w2\", w2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDdjU3X-3oo7",
        "outputId": "5236ed50-3848-4c04-e378-fdbafc44725b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o/p [0.86 0.84]\n",
            "activation [0.70266065 0.69846522]\n",
            "final output [0.71097897]\n",
            "loss [0.00792474]\n",
            "updated w2 [[0.40257072]\n",
            " [0.60255537]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class FeedForwardNN:\n",
        "    def __init__(self):\n",
        "        # Updated hidden layer weights and bias\n",
        "        self.w1 = np.array([\n",
        "            [0.2, 0.1],\n",
        "            [0.4, 0.3],\n",
        "            [0.6, 0.5]\n",
        "        ])\n",
        "        self.b1 = np.array([0.3, 0.4])\n",
        "\n",
        "        # Output layer\n",
        "        self.w2 = np.array([[0.2],\n",
        "                            [0.3]])\n",
        "        self.b2 = np.array([0.5])\n",
        "\n",
        "        self.lr = 0.1\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.z1 = np.dot(x, self.w1) + self.b1\n",
        "        self.a1 = self.sigmoid(self.z1)\n",
        "\n",
        "        self.z2 = np.dot(self.a1, self.w2) + self.b2\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "\n",
        "        return self.a2\n",
        "\n",
        "    def loss(self, y, y_pred):\n",
        "        return (y - y_pred) ** 2\n",
        "\n",
        "    def backward(self, y):\n",
        "        dloss_da2 = -2 * (y - self.a2)\n",
        "        da2_dz2 = self.a2 * (1 - self.a2)\n",
        "\n",
        "        grad_w2 = dloss_da2 * da2_dz2 * self.a1\n",
        "        grad_b2 = dloss_da2 * da2_dz2\n",
        "\n",
        "        self.w2 -= self.lr * grad_w2.reshape(2, 1)\n",
        "        self.b2 -= self.lr * grad_b2\n",
        "\n",
        "    def train(self, x, y):\n",
        "        y_pred = self.forward(x)\n",
        "        loss = self.loss(y, y_pred)\n",
        "        self.backward(y)\n",
        "        return y_pred, loss\n",
        "\n",
        "\n",
        "# -------- Execution --------\n",
        "x = np.array([0.2, 0.4, 0.6])\n",
        "y = 1\n",
        "\n",
        "model = FeedForwardNN()\n",
        "output, loss = model.train(x, y)\n",
        "\n",
        "print(\"Final Output:\", output)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Updated w2:\\n\", model.w2)\n",
        "print(\"Updated b2:\", model.b2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqF4stMzrlKs",
        "outputId": "1c83300a-988c-44ba-d200-a2e5c2727730"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Output: [0.70058218]\n",
            "Loss: [0.08965103]\n",
            "Updated w2:\n",
            " [[0.20882653]\n",
            " [0.30877383]]\n",
            "Updated b2: [0.51256158]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([[0.2, 0.4, 0.6]], dtype=np.float32)\n",
        "y = np.array([[1]], dtype=np.float32)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(\n",
        "        2,\n",
        "        activation='sigmoid',\n",
        "        input_shape=(3,),\n",
        "        kernel_initializer=tf.constant_initializer([\n",
        "            [0.2, 0.1],\n",
        "            [0.4, 0.3],\n",
        "            [0.6, 0.5]\n",
        "        ]),\n",
        "        bias_initializer=tf.constant_initializer([0.3, 0.4])\n",
        "    ),\n",
        "    tf.keras.layers.Dense(\n",
        "        1,\n",
        "        activation='sigmoid'\n",
        "    )\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
        "    loss='mean_squared_error'\n",
        ")\n",
        "\n",
        "model.fit(x, y, epochs=10, verbose=1)\n",
        "\n",
        "output = model.predict(x)\n",
        "print(\"Final Output:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtJlZWH6_qdq",
        "outputId": "746da8b9-afac-4722-df65-3018874c7592"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 438ms/step - loss: 0.2961\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.2806\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.2659\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.2519\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2386\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.2260\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.2142\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.2030\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.1925\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.1827\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "Final Output: [[0.58344513]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "x = torch.tensor([[0.2, 0.4, 0.6]], dtype=torch.float32)\n",
        "y = torch.tensor([[1.0]], dtype=torch.float32)\n",
        "\n",
        "class FeedForwardNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(3, 2)\n",
        "        self.fc2 = nn.Linear(2, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.fc1.weight.copy_(torch.tensor([\n",
        "                [0.2, 0.4, 0.6],\n",
        "                [0.1, 0.3, 0.5]\n",
        "            ]))\n",
        "            self.fc1.bias.copy_(torch.tensor([0.3, 0.4]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.sigmoid(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "model = FeedForwardNN()\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(x)\n",
        "    loss = criterion(output, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}: Loss = {loss.item():.6f}\")\n",
        "\n",
        "final_output = model(x)\n",
        "print(\"Final Output:\", final_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRbSUukgDkRe",
        "outputId": "99e148f5-cc7f-4b3b-e6a6-e0354bff03f9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 0.108582\n",
            "Epoch 2: Loss = 0.104385\n",
            "Epoch 3: Loss = 0.100426\n",
            "Epoch 4: Loss = 0.096690\n",
            "Epoch 5: Loss = 0.093161\n",
            "Epoch 6: Loss = 0.089827\n",
            "Epoch 7: Loss = 0.086673\n",
            "Epoch 8: Loss = 0.083688\n",
            "Epoch 9: Loss = 0.080861\n",
            "Epoch 10: Loss = 0.078182\n",
            "Final Output: tensor([[0.7250]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    }
  ]
}